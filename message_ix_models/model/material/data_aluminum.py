import os
from collections import defaultdict
from collections.abc import Iterable

import message_ix
import pandas as pd
from message_ix import make_df

from message_ix_models import ScenarioInfo
from message_ix_models.util import (
    broadcast,
    nodes_ex_world,
    package_data_path,
    same_node,
)

from .data_util import read_rel, read_timeseries
from .material_demand import material_demand_calc
from .util import (
    add_R12_column,
    combine_df_dictionaries,
    get_pycountry_iso,
    get_ssp_from_context,
    read_config,
)


def read_data_aluminum(
    scenario: message_ix.Scenario,
) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """Read and clean data from :file:`aluminum_techno_economic.xlsx`.

    Parameters
    ----------
    scenario: message_ix.Scenario
        Scenario instance to build aluminum on
    Returns
    -------
    tuple of three pd.DataFrames
        returns aluminum data in three separate groups
        time indepenendent parameters, relation parameters and time dependent parameters
    """

    # Ensure config is loaded, get the context
    s_info = ScenarioInfo(scenario)

    # Shorter access to sets configuration
    # sets = context["material"]["generic"]

    fname = "aluminum_techno_economic.xlsx"

    sheet_n = "data_R12" if "R12_CHN" in s_info.N else "data_R11"

    # Read the file
    data_alu = pd.read_excel(
        package_data_path("material", "aluminum", fname), sheet_name=sheet_n
    )

    # Drop columns that don't contain useful information
    data_alu = data_alu.drop(["Source", "Description"], axis=1)

    data_alu_rel = read_rel(scenario, "aluminum", None, "aluminum_techno_economic.xlsx")

    data_aluminum_ts = read_timeseries(
        scenario, "aluminum", None, "aluminum_techno_economic.xlsx"
    )

    # Unit conversion

    # At the moment this is done in the excel file, can be also done here
    # To make sure we use the same units

    return data_alu, data_alu_rel, data_aluminum_ts


def gen_data_alu_ts(data: pd.DataFrame, nodes: list) -> dict[str, pd.DataFrame]:
    """
    Generates time variable parameter data for aluminum sector
    Parameters
    ----------
    data: pd.DataFrame
        time variable data from input file
    nodes: list
        regions of model

    Returns
    -------
    dict[str, pd.DataFrame]
        key-value pairs of parameter names and parameter data
    """
    tec_ts = set(data.technology)  # set of tecs in timeseries sheet
    common = dict(
        time="year",
        time_origin="year",
        time_dest="year",
    )
    par_dict = defaultdict(list)
    for t in tec_ts:
        param_name = data.loc[(data["technology"] == t), "parameter"].unique()
        for p in set(param_name):
            val = data.loc[
                (data["technology"] == t) & (data["parameter"] == p),
                "value",
            ]
            # units = data.loc[
            #     (data["technology"] == t)
            #     & (data["parameter"] == p),
            #     "units",
            # ].values[0]
            mod = data.loc[
                (data["technology"] == t) & (data["parameter"] == p),
                "mode",
            ]
            yr = data.loc[
                (data["technology"] == t) & (data["parameter"] == p),
                "year",
            ]

            if p == "var_cost":
                df = make_df(
                    p,
                    technology=t,
                    value=val,
                    unit="t",
                    year_vtg=yr,
                    year_act=yr,
                    mode=mod,
                    **common,
                ).pipe(broadcast, node_loc=nodes)
            else:
                rg = data.loc[
                    (data["technology"] == t) & (data["parameter"] == p),
                    "region",
                ]
                df = make_df(
                    p,
                    technology=t,
                    value=val,
                    unit="t",
                    year_vtg=yr,
                    year_act=yr,
                    mode=mod,
                    node_loc=rg,
                    **common,
                )

            par_dict[p].append(df)
    return {par_name: pd.concat(dfs) for par_name, dfs in par_dict.items()}


def gen_data_alu_rel(data: pd.DataFrame, years: list) -> dict[str, pd.DataFrame]:
    par_dict = defaultdict(list)
    regions = set(data["Region"].values)
    for reg in regions:
        for r in data["relation"].unique():
            if r is None:
                break

            params = set(data.loc[(data["relation"] == r), "parameter"].values)

            # This relation should start from 2020...
            if r == "minimum_recycling_aluminum":
                modelyears_copy = years[:]
                if 2020 in modelyears_copy:
                    modelyears_copy.remove(2020)

                common_rel = dict(
                    year_rel=modelyears_copy,
                    year_act=modelyears_copy,
                    mode="M1",
                    relation=r,
                )
            else:
                # Use all the model years for other relations...
                common_rel = dict(
                    year_rel=years,
                    year_act=years,
                    mode="M1",
                    relation=r,
                )

            for par_name in params:
                if par_name == "relation_activity":
                    tec_list = data.loc[
                        ((data["relation"] == r) & (data["parameter"] == par_name)),
                        "technology",
                    ]

                    for tec in tec_list.unique():
                        val = data.loc[
                            (
                                (data["relation"] == r)
                                & (data["parameter"] == par_name)
                                & (data["technology"] == tec)
                                & (data["Region"] == reg)
                            ),
                            "value",
                        ].values[0]

                        df = make_df(
                            par_name,
                            technology=tec,
                            value=val,
                            unit="-",
                            node_loc=reg,
                            node_rel=reg,
                            **common_rel,
                        ).pipe(same_node)

                        par_dict[par_name].append(df)

                elif (par_name == "relation_upper") | (par_name == "relation_lower"):
                    val = data.loc[
                        (
                            (data["relation"] == r)
                            & (data["parameter"] == par_name)
                            & (data["Region"] == reg)
                        ),
                        "value",
                    ].values[0]

                    df = make_df(
                        par_name, value=val, unit="-", node_rel=reg, **common_rel
                    )

                    par_dict[par_name].append(df)
    return {par_name: pd.concat(dfs) for par_name, dfs in par_dict.items()}


def assign_input_outpt(
    split,
    param_name: str,
    regions: pd.DataFrame,
    val,
    t: str,
    rg: str,
    glb_reg: str,
    common: dict,
    yv_ya: pd.DataFrame,
    nodes,
):
    # Assign commodity and level names
    # Later mod can be added
    com = split[1]
    lev = split[2]

    if (param_name == "input") and (lev == "import"):
        df = make_df(
            param_name,
            technology=t,
            commodity=com,
            level=lev,
            value=val[regions[regions == rg].index[0]],
            unit="t",
            node_loc=rg,
            node_origin=glb_reg,
            **common,
        )

    elif (param_name == "output") and (lev == "export"):
        df = make_df(
            param_name,
            technology=t,
            commodity=com,
            level=lev,
            value=val[regions[regions == rg].index[0]],
            unit="t",
            node_loc=rg,
            node_dest=glb_reg,
            **common,
        )

    # Assign higher efficiency to younger plants
    elif (
        ((t == "soderberg_aluminum") or (t == "prebake_aluminum"))
        & (com == "electr")
        & (param_name == "input")
    ):
        # All the vÄ±ntage years
        year_vtg = sorted(set(yv_ya.year_vtg.values))
        # Collect the values for the combination of vintage and
        # active years.
        input_values_all = []
        for yr_v in year_vtg:
            # The initial year efficiency value
            input_values_temp = [val[regions[regions == rg].index[0]]]
            # Reduction after the vintage year
            year_vtg_filtered = list(filter(lambda op: op >= yr_v, year_vtg))
            # Filter the active model years
            year_act = yv_ya.loc[yv_ya["year_vtg"] == yr_v, "year_act"].values
            for i in range(len(year_vtg_filtered) - 1):
                input_values_temp.append(input_values_temp[i] * 1.1)

            act_year_no = len(year_act)
            input_values_temp = input_values_temp[-act_year_no:]
            input_values_all = input_values_all + input_values_temp

        df = make_df(
            param_name,
            technology=t,
            commodity=com,
            level=lev,
            value=input_values_all,
            unit="t",
            node_loc=rg,
            **common,
        ).pipe(same_node)

    else:
        df = make_df(
            param_name,
            technology=t,
            commodity=com,
            level=lev,
            value=val[regions[regions == rg].index[0]],
            unit="t",
            node_loc=rg,
            **common,
        ).pipe(same_node)

    # Copy parameters to all regions, when node_loc is not GLB
    if (len(regions) == 1) and (rg != glb_reg):
        df["node_loc"] = None
        df = df.pipe(broadcast, node_loc=nodes)  # .pipe(same_node)
        # Use same_node only for non-trade technologies
        if (lev != "import") and (lev != "export"):
            df = df.pipe(same_node)
    return df


def gen_data_alu_const(
    data: pd.DataFrame,
    config: dict,
    glb_reg: str,
    years: Iterable,
    yv_ya: pd.DataFrame,
    nodes: list[str],
):
    results = defaultdict(list)
    for t in config["technology"]["add"]:
        t = t.id
        params = data.loc[(data["technology"] == t), "parameter"].unique()
        # Obtain the active and vintage years
        av = data.loc[(data["technology"] == t), "availability"].values[0]
        years = [year for year in years if year >= av]
        yv_ya = yv_ya.loc[yv_ya.year_vtg >= av]
        common = dict(
            year_vtg=yv_ya.year_vtg,
            year_act=yv_ya.year_act,
            mode="M1",
            time="year",
            time_origin="year",
            time_dest="year",
        )
        # Iterate over parameters
        for par in params:
            # Obtain the parameter names, commodity,level,emission
            split = par.split("|")
            param_name = split[0]

            # Obtain the scalar value for the parameter
            val = data.loc[
                ((data["technology"] == t) & (data["parameter"] == par)),
                "value",
            ]

            regions = data.loc[
                ((data["technology"] == t) & (data["parameter"] == par)),
                "region",
            ]

            for rg in regions:
                # For the parameters which includes index names
                if len(split) > 1:
                    if (param_name == "input") | (param_name == "output"):
                        df = assign_input_outpt(
                            split,
                            param_name,
                            regions,
                            val,
                            t,
                            rg,
                            glb_reg,
                            common,
                            yv_ya,
                            nodes,
                        )

                    elif param_name == "emission_factor":
                        # Assign the emisson type
                        emi = split[1]

                        df = make_df(
                            param_name,
                            technology=t,
                            value=val[regions[regions == rg].index[0]],
                            emission=emi,
                            unit="t",
                            node_loc=rg,
                            **common,
                        )

                # Parameters with only parameter name
                else:
                    df = make_df(
                        param_name,
                        technology=t,
                        value=val[regions[regions == rg].index[0]],
                        unit="t",
                        node_loc=rg,
                        **common,
                    )

                # Copy parameters to all regions
                if (
                    (len(regions) == 1)
                    and len(set(df["node_loc"])) == 1
                    and list(set(df["node_loc"]))[0] != glb_reg
                ):
                    df["node_loc"] = None
                    df = df.pipe(broadcast, node_loc=nodes)

                results[param_name].append(df)
    return {par_name: pd.concat(dfs) for par_name, dfs in results.items()}


def gen_data_aluminum(
    scenario: message_ix.Scenario, dry_run: bool = False
) -> dict[str, pd.DataFrame]:
    """

    Parameters
    ----------
    scenario: message_ix.Scenario
        Scenario instance to build aluminum model on
    dry_run: bool
        *not implemented*
    Returns
    -------
    dict[pd.DataFrame]
        dict with MESSAGEix parameters as keys and parametrization as values
        stored in pd.DataFrame
    """
    context = read_config()
    config = context["material"]["aluminum"]

    # Information about scenario, e.g. node, year
    s_info = ScenarioInfo(scenario)
    ssp = get_ssp_from_context(context)
    # Techno-economic assumptions
    data_aluminum, data_aluminum_rel, data_aluminum_ts = read_data_aluminum(scenario)
    # List of data frames, to be concatenated together at end

    modelyears = s_info.Y
    yv_ya = s_info.yv_ya
    nodes = nodes_ex_world(s_info.N)
    global_region = [i for i in s_info.N if i.endswith("_GLB")][0]

    const_dict = gen_data_alu_const(
        data_aluminum, config, global_region, modelyears, yv_ya, nodes
    )

    parname = "demand"
    demand_dict = {}
    df_2025 = pd.read_csv(package_data_path("material", "aluminum", "demand_2025.csv"))
    df = material_demand_calc.derive_demand("aluminum", scenario, ssp=ssp)
    df = df[df["year"] != 2025]
    df = pd.concat([df_2025, df])
    demand_dict[parname] = df

    ts_dict = gen_data_alu_ts(data_aluminum_ts, nodes)
    ts_dict.update(gen_hist_new_cap())
    ts_dict = combine_df_dictionaries(ts_dict, gen_hist_act())

    rel_dict = gen_data_alu_rel(data_aluminum_rel, modelyears)

    trade_dict = gen_data_alu_trade(scenario)

    results_aluminum = combine_df_dictionaries(
        const_dict, ts_dict, rel_dict, demand_dict, trade_dict
    )
    return results_aluminum


def gen_mock_demand_aluminum(scenario: message_ix.Scenario) -> pd.DataFrame:
    s_info = ScenarioInfo(scenario)
    nodes = s_info.N
    nodes.remove("World")

    # Demand at product level (IAI Global Aluminum Cycle 2018)
    # Globally: 82.4 Mt
    # Domestic production + Import
    # AFR: No Data
    # CPA - China: 28.2 Mt
    # EEU / 2 + WEU / 2 = Europe 12.5 Mt
    # FSU: No data
    # LAM: South America: 2.5 Mt
    # MEA: Middle East: 2
    # NAM: North America: 14.1
    # PAO: Japan: 3
    # PAS/2 + SAS /2: Other Asia: 11.5 Mt
    # Remaining 8.612 Mt shared between AFR and FSU
    # This is used as 2020 data.

    # For R12: China and CPA demand divided by 0.1 and 0.9.

    # The order:
    # r = ['R12_AFR', 'R12_RCPA', 'R12_EEU', 'R12_FSU', 'R12_LAM', 'R12_MEA',\
    # 'R12_NAM', 'R12_PAO', 'R12_PAS', 'R12_SAS', 'R12_WEU',"R12_CHN"]

    if "R12_CHN" in nodes:
        nodes.remove("R12_GLB")
        sheet_n = "data_R12"
        region_set = "R12_"
        d = [3, 2, 6, 5, 2.5, 2, 13.6, 3, 4.8, 4.8, 6, 26]

    else:
        nodes.remove("R11_GLB")
        sheet_n = "data_R11"
        region_set = "R11_"
        d = [3, 28, 6, 5, 2.5, 2, 13.6, 3, 4.8, 4.8, 6]

    # SSP2 R11 baseline GDP projection
    gdp_growth = pd.read_excel(
        package_data_path("material", "other", "iamc_db ENGAGE baseline GDP PPP.xlsx"),
        sheet_name=sheet_n,
    )

    gdp_growth = gdp_growth.loc[
        (gdp_growth["Scenario"] == "baseline") & (gdp_growth["Region"] != "World")
    ].drop(["Model", "Variable", "Unit", "Notes", 2000, 2005], axis=1)

    gdp_growth["Region"] = region_set + gdp_growth["Region"]

    demand2020_al = (
        pd.DataFrame({"Region": nodes, "Val": d})
        .join(gdp_growth.set_index("Region"), on="Region")
        .rename(columns={"Region": "node"})
    )

    demand2020_al.iloc[:, 3:] = (
        demand2020_al.iloc[:, 3:]
        .div(demand2020_al[2020], axis=0)
        .multiply(demand2020_al["Val"], axis=0)
    )

    demand2020_al = pd.melt(
        demand2020_al.drop(["Val", "Scenario"], axis=1),
        id_vars=["node"],
        var_name="year",
        value_name="value",
    )

    return demand2020_al


def gen_data_alu_trade(scenario: message_ix.Scenario) -> dict[str, pd.DataFrame]:
    results = defaultdict(list)

    data_trade = pd.read_csv(
        package_data_path("material", "aluminum", "aluminum_trade.csv")
    )

    data_trade.drop_duplicates()

    s_info = ScenarioInfo(scenario)

    yv_ya = s_info.yv_ya
    year_all = yv_ya["year_vtg"].unique()

    data_trade = data_trade[data_trade["Year"].isin(year_all)]

    # Divide R12_WEU as 0.7 WEU, 0.3 EEU.
    data_trade.loc[(data_trade["Region"] == "Europe"), "Value"] *= 0.7
    data_trade.loc[(data_trade["Region"] == "Europe"), "Region"] = "West Europe"

    data_trade_eeu = data_trade.loc[data_trade["Region"] == "West Europe"].copy(
        deep=True
    )
    data_trade_eeu["Value"] *= 0.3 / 0.7
    data_trade_eeu["Region"] = "East Europe"

    data_trade = pd.concat([data_trade, data_trade_eeu])

    # Sum Japan and Oceania as PAO

    condition = (data_trade["Region"] == "Japan") | (data_trade["Region"] == "Oceania")
    data_trade_pao = data_trade.loc[condition]
    data_trade_pao = (
        data_trade_pao.groupby(["Variable", "Year"])["Value"].sum().reset_index()
    )

    data_trade_pao["Region"] = "Pacific OECD"
    data_trade = pd.concat([data_trade, data_trade_pao])
    condition_updated = (data_trade["Region"] == "Japan") | (
        data_trade["Region"] == "Oceania"
    )
    data_trade = data_trade.drop(data_trade[condition_updated].index)

    data_trade.reset_index(drop=True, inplace=True)

    # Divide Other Asia 50-50 to SAS and PAS

    data_trade.loc[(data_trade["Region"] == "Other Asia"), "Value"] *= 0.5
    data_trade.loc[(data_trade["Region"] == "Other Asia"), "Region"] = "South Asia"

    data_trade_pas = data_trade[data_trade["Region"] == "South Asia"].copy(deep=True)
    data_trade_pas["Region"] = "Other Pacific Asia"

    data_trade = pd.concat([data_trade, data_trade_pas])

    # Divide Other Producing Regions 50-50s as Africa and FSU

    data_trade.loc[(data_trade["Region"] == "Other Producers"), "Value"] *= 0.5
    data_trade.loc[(data_trade["Region"] == "Other Producers"), "Region"] = "Africa"

    data_trade_fsu = data_trade[data_trade["Region"] == "Africa"].copy(deep=True)
    data_trade_fsu["Region"] = "Former Soviet Union"

    data_trade = pd.concat([data_trade, data_trade_fsu])

    # Drop non-producers

    condition = data_trade["Region"] == "Non Producers"
    data_trade = data_trade.drop(data_trade[condition].index)

    s_info = ScenarioInfo(scenario)

    region_tag = "R12_" if "R12_CHN" in s_info.N else "R11_"
    china_mapping = "R12_CHN" if "R12_CHN" in s_info.N else "R11_CPA"

    region_mapping = {
        "China": china_mapping,
        "West Europe": region_tag + "WEU",
        "East Europe": region_tag + "EEU",
        "Pacific OECD": region_tag + "PAO",
        "South Asia": region_tag + "SAS",
        "Other Pacific Asia": region_tag + "PAS",
        "Africa": region_tag + "AFR",
        "Former Soviet Union": region_tag + "FSU",
        "Middle East": region_tag + "MEA",
        "North America": region_tag + "NAM",
        "South America": region_tag + "LAM",
    }

    # Add the data as historical_activity

    data_trade = data_trade.replace(region_mapping)
    data_trade.rename(
        columns={"Region": "node_loc", "Year": "year_act", "Value": "value"},
        inplace=True,
    )

    # Trade is at the product level.
    # For imports this corresponds to: USE|Inputs|Imports

    data_import = data_trade[data_trade["Variable"] == "USE|Inputs|Imports"]
    data_import_hist = data_import[data_import["year_act"] <= 2015].copy(deep=True)
    data_import_hist["technology"] = "import_aluminum"
    data_import_hist["mode"] = "M1"
    data_import_hist["time"] = "year"
    data_import_hist["unit"] = "-"
    data_import_hist.drop(["Variable"], axis=1, inplace=True)
    data_import_hist.reset_index(drop=True)

    # For exports this corresponds to: MANUFACTURING|Outputs|Exports

    data_export = data_trade[data_trade["Variable"] == "MANUFACTURING|Outputs|Exports"]
    data_export_hist = data_export[data_export["year_act"] <= 2015].copy(deep=True)
    data_export_hist["technology"] = "export_aluminum"
    data_export_hist["mode"] = "M1"
    data_export_hist["time"] = "year"
    data_export_hist["unit"] = "-"
    data_export_hist.drop(["Variable"], axis=1, inplace=True)
    data_export_hist.reset_index(drop=True)

    results["historical_activity"].append(data_export_hist)
    results["historical_activity"].append(data_import_hist)

    # Add data as historical_new_capacity for export

    for r in data_export_hist["node_loc"].unique():
        df_hist_cap = data_export_hist[data_export_hist["node_loc"] == r]
        df_hist_cap = df_hist_cap.sort_values(by="year_act")
        df_hist_cap["value_difference"] = df_hist_cap["value"].diff()
        df_hist_cap["value_difference"] = df_hist_cap["value_difference"].fillna(
            df_hist_cap["value"]
        )
        df_hist_cap["historical_new_capacity"] = df_hist_cap["value_difference"] / 5

        df_hist_cap = df_hist_cap.drop(
            columns=["mode", "time", "value", "value_difference"], axis=1
        )
        df_hist_cap.rename(
            columns={"historical_new_capacity": "value", "year_act": "year_vtg"},
            inplace=True,
        )

        df_hist_cap["value"] = df_hist_cap["value"].apply(lambda x: 0 if x < 0 else x)
        df_hist_cap["unit"] = "-"
        results["historical_new_capacity"].append(df_hist_cap)

    # For China fixing 2020 and 2025 values

    import_chn = data_import[
        (data_import["year_act"] == 2020) & (data_import["node_loc"] == "R12_CHN")
    ]

    export_chn = data_export[
        (data_export["year_act"] == 2020) & (data_export["node_loc"] == "R12_CHN")
    ]

    # Merge the DataFrames on 'node_loc' and 'year'
    merged_df = pd.merge(
        import_chn,
        export_chn,
        on=["node_loc", "year_act"],
        suffixes=("_import", "_export"),
    )

    # Subtract the 'value_import' from 'value_export' to get net export value
    merged_df["value"] = merged_df["value_export"] - merged_df["value_import"]

    # Select relevant columns for the final DataFrame
    bound_act_net_export_chn = merged_df[["node_loc", "year_act", "value"]].copy(
        deep=True
    )

    bound_act_net_export_chn["technology"] = "export_aluminum"
    bound_act_net_export_chn["mode"] = "M1"
    bound_act_net_export_chn["time"] = "year"
    bound_act_net_export_chn["unit"] = "-"

    bound_act_net_export_chn_2025 = bound_act_net_export_chn.replace({2020: 2025})

    results["bound_activity_up"].append(bound_act_net_export_chn)
    results["bound_activity_lo"].append(bound_act_net_export_chn)
    results["bound_activity_up"].append(bound_act_net_export_chn_2025)
    results["bound_activity_lo"].append(bound_act_net_export_chn_2025)

    return {par_name: pd.concat(dfs) for par_name, dfs in results.items()}


def gen_hist_new_cap():
    df_cap = pd.read_excel(
        package_data_path("material", "aluminum", "smelters-with 2022 projection.xls"),
        sheet_name="Sheet1",
        skipfooter=23,
    ).rename(columns={"Unnamed: 0": "Country", "Unnamed: 1": "Region"})
    df_cap.Technology = df_cap.Technology.fillna("unknown")
    df_cap = df_cap[~df_cap[1995].isna()]
    df_cap.Country = df_cap["Country"].ffill()
    df_cap["ISO"] = df_cap.Country.apply(
        lambda c: get_pycountry_iso(
            c,
            {
                "Surinam": "SUR",
                "Trinidad": "TTO",
                "Quatar": "QAT",
                "Turkey": "TUR",
                "UAE": "ARE",
                "Gernamy": "DEU",
                "Azerbaydzhan": "AZE",
                "Russia": "RUS",
                "Tadzhikistan": "TJK",
                "UK": "GBR",
                "Total": "World",
                "Bosnia": "BIH",
            },
        )
    )
    df_cap = add_R12_column(
        df_cap, file_path=package_data_path("node", "R12.yaml"), iso_column="ISO"
    )

    # generate historical_new_capacity for soderberg
    df_cap_ss = df_cap[
        (df_cap.Technology.str.contains("SS") & ~(df_cap.Technology.str.contains("PB")))
        & ~(df_cap.Technology.str.contains("HAL"))
        & ~(df_cap.Technology.str.contains("P"))
    ]
    df_cap_ss_r12 = df_cap_ss.groupby("R12").sum(numeric_only=True)
    sample = df_cap_ss_r12[df_cap_ss_r12[df_cap_ss_r12.columns[-1]] != 0][
        [i for i in range(1995, 2020, 5)] + [2019]
    ]
    hist_new_cap_ss = compute_differences(sample, 1995) / 10**6
    hist_new_cap_ss = hist_new_cap_ss.rename(columns={2019: 2020})
    hist_new_cap_ss = (
        hist_new_cap_ss.reset_index()
        .melt(id_vars="R12", var_name="year_vtg")
        .assign(unit="Mt", technology="soderberg_aluminum")
        .rename(columns={"R12": "node_loc"})
    )

    # generate historical_new_capacity for prebake
    df_cap_pb = df_cap.loc[df_cap.index.difference(df_cap_ss.index)]
    df_cap_pb_r12 = df_cap_pb.groupby("R12").sum(numeric_only=True)
    sample = df_cap_pb_r12[[i for i in range(1995, 2020, 5)] + [2019]]
    hist_new_cap_pb = compute_differences(sample, 1995) / 10**6
    hist_new_cap_pb = hist_new_cap_pb.rename(columns={2019: 2020})
    hist_new_cap_pb = (
        hist_new_cap_pb.reset_index()
        .melt(id_vars="R12", var_name="year_vtg")
        .assign(unit="Mt", technology="prebake_aluminum")
        .rename(columns={"R12": "node_loc"})
    )

    return {"historical_new_capacity": pd.concat([hist_new_cap_ss, hist_new_cap_pb])}


def compute_differences(df, ref_col):
    # Initialize a DataFrame to store differences
    differences = df[ref_col].to_frame()

    # Start with the reference column
    ref_values = df[ref_col].copy()

    for col in df.columns:
        if col == ref_col:
            continue  # Skip the reference column

        # Compute differences
        diff = df[col] - ref_values
        diff[diff <= 0] = 0  # Keep only positive differences

        # Store differences
        differences[col] = diff

        # Update the reference column where the current column is greater
        ref_values = ref_values.where(df[col] <= ref_values, df[col])

    return differences


def load_bgs_data():
    bgs_data_path = package_data_path("material", "aluminum", "bgs_data")

    dfs = []

    for fname in os.listdir(bgs_data_path):
        if not fname.endswith(".xlsx"):
            continue
        # read and format BGS data
        df_prim = pd.read_excel(bgs_data_path + fname, skipfooter=9, skiprows=1)
        year_cols = df_prim.columns[2::2]
        df_prim = df_prim[
            [df_prim.columns.tolist()[0]] + df_prim.columns[3::2].tolist()
        ]
        df_prim.columns = ["Country"] + [int(i) for i in year_cols]
        df_prim["ISO"] = df_prim["Country"].apply(
            lambda x: get_pycountry_iso(
                x,
                {
                    "Turkey": "TUR",
                    "Russia": "RUS",
                    "Bosnia & Herzegovina": "BIH",
                    "Czechoslovakia": "CSK",
                    "German Democratic Rep": "DEU",
                    "Korea (Rep. of)": "KOR",
                    "Soviet Union": "RUS",
                    "Korea, Dem. P.R. of": "PRK",
                    "Serbia and Montenegro": "SRB",
                    "Yugoslavia": "YUG",
                    "German Federal Republic": "DEU",
                },
            )
        )
        df_prim.drop("Country", axis=1, inplace=True)
        for year in [i for i in df_prim.columns if isinstance(i, int)]:
            df_prim[year] = pd.to_numeric(df_prim[year], errors="coerce")
        dfs.append(df_prim)

    df_prim = dfs[0].groupby("ISO").sum()
    for _df in dfs[1:]:
        df_prim = _df.groupby("ISO").sum().join(df_prim, how="outer")
    df_prim = df_prim.dropna(how="all")
    df_prim = df_prim[sorted(df_prim.columns)]

    df_prim.reset_index(inplace=True)

    # add R12 column
    df_prim = add_R12_column(
        df_prim.rename(columns={"ISO": "COUNTRY"}),
        package_data_path("node", "R12_worldsteel.yaml"),
    )
    df_prim.rename(columns={"COUNTRY": "ISO"}, inplace=True)

    return df_prim


def gen_hist_act():
    df_prim = load_bgs_data()
    df_prim_r12 = df_prim.groupby("R12").sum(numeric_only=True).div(10**6)

    # Soderberg
    df_ss_act = df_prim_r12[[2015, 2020]].copy(deep=True)
    # calculate historical production with soderberg electrodes in the only 3 regions
    #   that still have soderberg capacity (based on capacity data from genisim)
    df_ss_act.loc["R12_WEU"] *= 0.025
    df_ss_act.loc["R12_LAM"] *= 0.25
    df_ss_act.loc["R12_FSU"] *= 0.65
    df_ss_act.loc[["R12_FSU", "R12_LAM", "R12_WEU"]]
    df_ss_act = (
        df_ss_act.reset_index()
        .rename(columns={"R12": "node_loc"})
        .melt(id_vars="node_loc", var_name="year_act")
    )
    df_ss_act = df_ss_act.assign(
        technology="soderberg_aluminum", mode="M1", time="year", unit="Mt/yr"
    )
    df_ss_act = make_df("historical_activity", **df_ss_act)

    # Prebake
    df_pb_act = df_prim_r12[[2015, 2020]].copy(deep=True)
    # deduct historical production with soderberg electrodes in the only 3 regions that
    #   still have soderberg capacity (based on capacity data from genisim) to get
    #   production with prebaked electrodes
    df_pb_act.loc["R12_WEU"] *= 1 - 0.025
    df_pb_act.loc["R12_LAM"] *= 1 - 0.25
    df_pb_act.loc["R12_FSU"] *= 1 - 0.65
    df_pb_act = (
        df_pb_act.reset_index()
        .rename(columns={"R12": "node_loc"})
        .melt(id_vars="node_loc", var_name="year_act")
    )
    df_pb_act = df_pb_act.assign(
        technology="prebake_aluminum", mode="M1", time="year", unit="Mt/yr"
    )
    df_pb_act = make_df("historical_activity", **df_pb_act)

    par_dict = {}
    par_dict["bound_activity_up"] = pd.concat(
        [
            df_pb_act[df_pb_act["year_act"] == 2020],
            df_ss_act[df_ss_act["year_act"] == 2020],
        ]
    )
    par_dict["bound_activity_lo"] = par_dict["bound_activity_up"].copy(deep=True)
    return par_dict
